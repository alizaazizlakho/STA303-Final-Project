---
title: "Data preparation"
output:
  pdf_document: default
---

# Instructions

- You only need to submit the .Rmd of this file, not a PDF.

- You should __comment__ your code clearly to show what you've done to prepare the data.

- The purpose of this file is to use the data in the `data-raw` folder to create the data you will use in the report. The data you will use in the report should be saved in the `data` folder. It is good professional practice to make sure you're never directly modifying your raw data, but instead creating new datasets based on merges/manipulations that you need to reuse.

- Make sure you've taken a look at the hints for the web scraping and census API. 

- You may find the `write_rds()` function from the `readr` package helpful (it is loaded as part of the `tidyverse`).

- You do not need to keep the structure below.

# Set up

```{r, libraries}
# Set up any libraries you need
# Note: In adapting this for your code, 
# please ensure all libraries are in a setup chunk at the beginning

# These are the libraries I find useful for webscraping
library(tidyverse)
library(polite)
library(rvest)
library(haven)
library(cancensus)
```

# Loading client data

```{r}
# install.packages("haven")

postcode <- readRDS("data-raw/break_glass_in_case_of_emergency.Rds")
cust_dev <- readRDS("~/sta303-w22-final-project-template/data-raw/cust_dev.Rds")
cust_sleep <- readRDS("~/sta303-w22-final-project-template/data-raw/cust_sleep.Rds")
customer <- readRDS("~/sta303-w22-final-project-template/data-raw/customer.Rds")
device <- readRDS("~/sta303-w22-final-project-template/data-raw/device.Rds")

```

# Getting external data

## Web scraping industry data

```{r}
url <- "https://fitnesstrackerinfohub.netlify.app/"

# Make sure this code is updated appropriately to provide 
# informative user_agent details
target <- bow(url,
              user_agent = "kyra.chow@utoronto.ca for STA303/1002 project",
              force = TRUE)

# Any details provided in the robots text on crawl delays and 
# which agents are allowed to scrape
target

html <- scrape(target)

device_data <- html %>% 
  html_elements("table") %>% 
  html_table() %>% 
  pluck(1) # added, in case you're getting a list format


```

# Census API

```{r}
#install.packages("cancensus")

options(cancensus.api_key = "CensusMapper_a7378c10c7d7ca6abebf450419b73937",
        cancensus.cache_path = "cache") # this sets a folder for your cache


# get all regions as at the 2016 Census (2020 not up yet)
regions <- list_census_regions(dataset = "CA16")

regions_filtered <-  regions %>% 
  filter(level == "CSD") %>% # Figure out what CSD means in Census data
  as_census_region_list()

# This can take a while
# We want to get household median income
census_data_csd <- get_census(dataset='CA16', regions = regions_filtered,
                          vectors=c("v_CA16_2397"), 
                          level='CSD', geo_format = "sf")

# Simplify to only needed variables
median_income <- census_data_csd %>% 
  as_tibble() %>% 
  select(CSDuid = GeoUID, contains("median"), Population) %>% 
  mutate(CSDuid = parse_number(CSDuid)) %>% 
  rename(hhld_median_inc = 2)

```
# Question 1
```{r}
#Merging customer data with device data
updated_cust <- merge(customer, cust_dev, by = "cust_id") %>% 
  select(-c(pronouns, emoji_modifier)) 

#Merging customer the previous data with device data
updated_dev <- merge(updated_cust, device, by = "dev_id")

#Updating the data with adding postal code infromation
updated_data1 <- merge(updated_dev, postcode, by.x = "postcode", by.y = "PC")

#Updating the data including median data
updated_data2 <- merge(updated_data1, median_income, by = "CSDuid")

#Creating data that answers the first question
q1_data <- updated_data2 %>% 
  mutate(new_old = ifelse(device_name == "Active" | device_name == "Alpha", "New", "Old"))

#Cleaning the data
q1_data <- q1_data %>% 
  mutate(age = round(as.numeric(difftime(Sys.Date(), dob, units="weeks"))/52.25, 0)) %>% 
  mutate(age_range = cut(age, breaks = c(15, 30, 45, 60, 75, 90, 105), right = FALSE))

q1_data <- q1_data %>% 
  mutate(income_range = cut(hhld_median_inc, breaks = c(40000, 80000, 120000, 160000, 200000), right = FALSE))

q1_data <- q1_data %>% 
  select(c(cust_id, new_old, age, age_range, hhld_median_inc, income_range, sex, Population))

q1_data <- q1_data %>% 
  mutate(new_old = fct_relevel(new_old, "New", "Old"))

#Saving the data as it will be used later
saveRDS(q1_data, file="data/q1_data.Rds")
```


# Question 2

```{r}
#merge the device customer device data with the device information
cust_data_updated <- cust_dev %>%
  left_join(device, by = "dev_id")

#merge the data with the sleep information
cust_data_updated <- cust_data_updated %>%
 right_join(cust_sleep, by = "cust_id")

#merge the data with the customer data to get the skin tone
cust_data_updated <- cust_data_updated %>%
  right_join(customer, by = "cust_id")

cust_data_updated$age = year(cust_data_updated$date) - year(cust_data_updated$dob)

#select the variables of interest and clean the data 
cust_data_updated <- cust_data_updated %>%
  select(cust_id, duration, flags, age, sex, emoji_modifier) %>%
  na.omit()

#rescale age
cust_data_updated1 <- cust_data_updated %>%
  transform(age = scales::rescale(age))
```

